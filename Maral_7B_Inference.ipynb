{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Maral 7B Inference Notebook\n",
        "\n",
        "<p align=\"center\">\n",
        " <img src=\"https://huggingface.co/MaralGPT/Maral-7B-alpha-1/resolve/main/maral-7b-announce.png\" width=256 height=256/>\n",
        "</p>\n",
        "\n",
        "## About Maral\n",
        "\n",
        "Maral is just a new large lanugage model, specializing on the Persian language. This model is based on Mistral and trained an Alpaca Persian dataset. This model is one of the few efforts in Persian speaking scene in order to bring our language to a new life in the era of AI.\n",
        "\n",
        "Also, since Maral is based on Mistral, it's capable of producing English answers as well.\n",
        "\n",
        "## Our Team\n",
        "\n",
        "* Muhammadreza Haghiri ([Website](https://haghiri75.com/en) - [Github](https://github.com/prp-e) - [LinkedIn](https://www.linkedin.com/in/muhammadreza-haghiri-1761325b))\n",
        "* Mahi Mohrechi ([Website](https://mohrechi-portfolio.vercel.app/) - [Github](https://github.com/f-mohrechi) - [LinkedIn](https://www.linkedin.com/in/faeze-mohrechi/))"
      ],
      "metadata": {
        "id": "7HgBfG4bVzMt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Needed libraries\n",
        "\n",
        "Since the model is loaded in 8 bit quantization mode on free colab, you need `bitsandbytes`. If you do own a better GPU, go with full 16 bit quantization."
      ],
      "metadata": {
        "id": "LCcxfXT2XdEL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zNtVtI3RSH4Z"
      },
      "outputs": [],
      "source": [
        "!pip install transformers accelerate bitsandbytes -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
        "import torch"
      ],
      "metadata": {
        "id": "g6c2XDlfSgAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Model"
      ],
      "metadata": {
        "id": "sAr9h2aIXtK5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name_or_id = \"MaralGPT/Maral-7B-alpha-1\""
      ],
      "metadata": {
        "id": "zGF7bfgdSm9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(model_name_or_id, torch_dtype=torch.float16, device_map=\"auto\", low_cpu_mem_usage=True, load_in_8bit=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name_or_id)"
      ],
      "metadata": {
        "id": "AOk3ZT8fSzxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Structure"
      ],
      "metadata": {
        "id": "w1cejoSyXxra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLoXQ6uFS2sd",
        "outputId": "0c680ba7-9948-4a4e-c4e0-88e9480e3335"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MistralForCausalLM(\n",
              "  (model): MistralModel(\n",
              "    (embed_tokens): Embedding(32000, 4096)\n",
              "    (layers): ModuleList(\n",
              "      (0-31): 32 x MistralDecoderLayer(\n",
              "        (self_attn): MistralAttention(\n",
              "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "          (rotary_emb): MistralRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): MistralMLP(\n",
              "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
              "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
              "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
              "          (act_fn): SiLUActivation()\n",
              "        )\n",
              "        (input_layernorm): MistralRMSNorm()\n",
              "        (post_attention_layernorm): MistralRMSNorm()\n",
              "      )\n",
              "    )\n",
              "    (norm): MistralRMSNorm()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt Format\n",
        "\n",
        "This model, uses _Guanaco_ format, which is like this:\n",
        "\n",
        "```\n",
        "### Human: <prompt>\n",
        "### Assistant: <answer>\n",
        "```\n",
        "\n",
        "So in the below cell, you can easily modify the prompt without messing with the format."
      ],
      "metadata": {
        "id": "qXwNn5ynyBKr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"در سال ۱۹۹۶ چه کسی رییس جمهور آمریکا بود؟\"\n",
        "prompt = f\"### Human:{prompt}\\n### Assistant:\""
      ],
      "metadata": {
        "id": "eDKNRtBvTtlO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")"
      ],
      "metadata": {
        "id": "D2LZicvNTwwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generation Config\n",
        "\n",
        "This cell, is a simple and easy way to tweak the configurations for text generation."
      ],
      "metadata": {
        "id": "ni3F1Sm1yYz_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generation_config = GenerationConfig(\n",
        "    do_sample=True,\n",
        "    top_k=1,\n",
        "    temperature=0.5,\n",
        "    max_new_tokens=100,\n",
        "    pad_token_id=tokenizer.eos_token_id\n",
        ")"
      ],
      "metadata": {
        "id": "8NK_6nTcTzn-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(**inputs, generation_config=generation_config)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PuDza4_T1iV",
        "outputId": "24d78bcf-39ea-4bda-fb48-6555e0fdb1dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Human:در سال ۱۹۹۶ چه کسی رییس جمهور آمریکا بود؟\n",
            "### Assistant: در سال 1996 بیل کلنتن رییس جمهور آمریکا بود.\n",
            "### Assistant: بیل کلنتن در سال 1992 به دومین رییس جمهور آمریکا انت\n"
          ]
        }
      ]
    }
  ]
}